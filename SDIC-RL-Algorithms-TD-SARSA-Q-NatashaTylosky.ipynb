{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./python\") \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Environment Loops for SuttonSimplest and CliffWalking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sutton Simplest MDP environment\n",
    "States = {A,B} cells ids <br>\n",
    "Actions = {1 in pyhton 0,  2 in python 1} <br>\n",
    "<img style=\"float: left;\" src=\"imgs/SuttonSimple.png\" width=\"320\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: 0\n",
      "New State: 0      Reward: 10\n",
      "New State: 0      Reward: -10\n",
      "New State: 0      Reward: -10\n",
      "New State: 0      Reward: 10\n",
      "New State: 1      Reward: -10\n",
      "New State: 0      Reward: 20\n",
      "New State: 1      Reward: -10\n",
      "New State: 1      Reward: 20\n",
      "New State: 0      Reward: 20\n",
      "New State: 1      Reward: -10\n",
      "Total obtained reward was:  30\n"
     ]
    }
   ],
   "source": [
    "env = Environment.Environment(\"SuttonSimplest\")\n",
    "print(\"Initial State: %d\"%env.reset())\n",
    "\n",
    "isteps, sum_reward, done = 0, 0, False\n",
    "observation = env.reset()\n",
    "while not done and isteps < 10:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"New State: \" + str(observation) + \"      Reward: \" + str(reward))\n",
    "    sum_reward += reward\n",
    "    isteps += 1\n",
    "print(\"Total obtained reward was: \", sum_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cliff Environment from the Sutton and Barto Book:  cliff_walking.py file\n",
    "States = {0,...,33} cells ids <br>\n",
    "Actions = {UP 0,RIGHT 1,DOWN 2,LEFT 3} <br>\n",
    "<img style=\"float: left;\" src=\"imgs/cliff_env.png\" width=\"320\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: 36\n",
      "New State: 37      Reward: -100.0\n",
      "Total obtained reward was:  -100.0\n"
     ]
    }
   ],
   "source": [
    "env = Environment.Environment(\"CliffWalking\")\n",
    "print(\"Initial State: %d\"%env.reset())\n",
    "\n",
    "sum_reward, done = 0, False\n",
    "observation = env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"New State: \" + str(observation) + \"      Reward: \" + str(reward))\n",
    "    sum_reward += reward\n",
    "    \n",
    "print(\"Total obtained reward was: \", sum_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD policy evaluation - Value function V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V_Class():  \n",
    "    \"\"\" Class to store the state Value function\n",
    "        V(s) = expected future discounted reward from s onwards (the return Gt)\n",
    "        Stores it as a dictionnary and adds states as encounteded (get method)\n",
    "        Two methods: get and set\n",
    "    \"\"\"\n",
    "    def __init__(self):       \n",
    "        self.f = {}\n",
    "        \n",
    "    def get(self, s):        \n",
    "        if(s not in self.f):\n",
    "            self.f[s] = 0             \n",
    "        return self.f[s]\n",
    "    \n",
    "    def set(self, s, y):\n",
    "        self.f[s] = y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Value Function evaluation for Random Policy\n",
      "0 -22.55\n",
      "1 -24.28\n",
      "2 -25.1\n",
      "3 -23.86\n",
      "4 -24.79\n",
      "5 -23.47\n",
      "6 -22.02\n",
      "7 -26.36\n",
      "8 -19.14\n",
      "9 -10.58\n",
      "10 -5.37\n",
      "11 -1.83\n",
      "12 -25.03\n",
      "13 -36.7\n",
      "14 -36.35\n",
      "15 -50.21\n",
      "16 -32.67\n",
      "17 -31.31\n",
      "18 -30.67\n",
      "19 -48.81\n",
      "20 -20.64\n",
      "21 -14.88\n",
      "22 -13.52\n",
      "23 -4.04\n",
      "24 -51.58\n",
      "25 -60.79\n",
      "26 -68.98\n",
      "27 -76.73\n",
      "28 -55.45\n",
      "29 -51.78\n",
      "30 -70.05\n",
      "31 -73.55\n",
      "32 -65.45\n",
      "33 -30.43\n",
      "34 -47.26\n",
      "35 -15.59\n",
      "36 -76.24\n",
      "37 0\n",
      "38 0\n",
      "39 0\n",
      "40 0\n",
      "41 0\n",
      "42 0\n",
      "43 0\n",
      "44 0\n",
      "45 0\n",
      "46 0\n",
      "47 0\n"
     ]
    }
   ],
   "source": [
    "env = Environment.Environment(\"CliffWalking\")\n",
    "V = V_Class()\n",
    "\n",
    "iepisode = 0\n",
    "while iepisode < 1000:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        new_Vs = V.get(obs) + 0.3*(reward + 0.9*V.get(new_obs) - V.get(obs)) \n",
    "        V.set(obs,new_Vs)\n",
    "        obs = new_obs\n",
    "        \n",
    "    iepisode += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"State Value Function evaluation for Random Policy\")\n",
    "\n",
    "for key, value in sorted(V.f.items()):\n",
    "    print(key, round(value,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD policy evaluation (state, action) Value function Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Class():  \n",
    "    \"\"\" Class to store the state action Value function Q(s,a)\n",
    "        Q(s,a) = expected future discounted reward from s taking a onwards \n",
    "        Stores it as a dictionnary and adds states as encounteded with all its actions\n",
    "        Two methods: get and set\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env):       \n",
    "        self.nactions = env.action_space.n\n",
    "        self.f = {}\n",
    "        \n",
    "    def get(self, s, a=None):        \n",
    "        if(s not in self.f):\n",
    "            self.f[s] = [0 for _ in range(self.nactions)]  \n",
    "        return self.f[s] if a is None else self.f[s][a]\n",
    "    \n",
    "    def set(self, s, a, y):\n",
    "        self.f[s][a] = y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Value Function evaluation for Random Policy\n",
      "0 [-26.7, -24.11, -36.01, -24.81]\n",
      "1 [-26.71, -23.95, -32.11, -26.43]\n",
      "2 [-25.3, -31.71, -29.26, -23.63]\n",
      "3 [-29.99, -18.41, -53.76, -23.69]\n",
      "4 [-14.31, -13.84, -23.25, -22.41]\n",
      "5 [-10.2, -6.4, -16.04, -17.82]\n",
      "6 [-8.42, -2.13, -11.27, -10.17]\n",
      "7 [-2.73, -1.35, -11.18, -3.09]\n",
      "8 [-0.94, -1.24, -1.55, -1.59]\n",
      "9 [-0.95, -1.84, -0.94, -0.41]\n",
      "10 [-0.3, -0.51, -3.74, -0.54]\n",
      "11 [-0.3, 0, -0.46, -0.44]\n",
      "12 [-25.82, -42.79, -45.97, -33.6]\n",
      "13 [-25.54, -35.19, -49.58, -33.66]\n",
      "14 [-23.88, -39.63, -44.26, -29.6]\n",
      "15 [-19.42, -31.4, -76.65, -31.54]\n",
      "16 [-18.57, -22.82, -40.8, -27.92]\n",
      "17 [-11.73, -18.26, -43.54, -28.41]\n",
      "18 [-6.78, -7.88, -57.38, -13.27]\n",
      "19 [-0.75, -1.92, -20.13, -21.46]\n",
      "20 [-1.12, -2.21, -0.65, -1.42]\n",
      "21 [-1.43, -0.72, -17.06, -1.49]\n",
      "22 [-0.97, -0.65, -20.2, -1.67]\n",
      "23 [0, -0.74, 0, -1.16]\n",
      "24 [-31.17, -37.1, -80.33, -33.05]\n",
      "25 [-30.36, -58.43, -100.0, -36.83]\n",
      "26 [-32.08, -56.11, -100.0, -54.06]\n",
      "27 [-38.4, -42.69, -100.0, -62.42]\n",
      "28 [-26.16, -64.16, -99.99, -61.45]\n",
      "29 [-11.37, -39.87, -99.53, -61.15]\n",
      "30 [-4.91, -13.3, -99.03, -38.18]\n",
      "31 [-11.86, -0.51, -83.19, -46.47]\n",
      "32 [-0.91, -0.38, -30.0, -8.33]\n",
      "33 [-2.88, -0.71, -65.7, -0.59]\n",
      "34 [-0.38, -0.51, -65.7, -0.67]\n",
      "35 [0, 0, -0.3, -0.38]\n",
      "36 [-42.22, -100.0, -75.28, -56.16]\n",
      "37 [0, 0, 0, 0]\n",
      "38 [0, 0, 0, 0]\n",
      "39 [0, 0, 0, 0]\n",
      "40 [0, 0, 0, 0]\n",
      "41 [0, 0, 0, 0]\n",
      "42 [0, 0, 0, 0]\n",
      "43 [0, 0, 0, 0]\n",
      "44 [0, 0, 0, 0]\n",
      "45 [0, 0, 0, 0]\n",
      "46 [0, 0, 0, 0]\n",
      "47 [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "env = Environment.Environment(\"CliffWalking\")\n",
    "Q = Q_Class(env)\n",
    "\n",
    "iepisode = 0\n",
    "while iepisode < 1000:\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    a = env.action_space.sample()\n",
    "    while not done:        \n",
    "        s_new, reward, done, info = env.step(a)\n",
    "\n",
    "        a_new = env.action_space.sample()\n",
    "        #print(s_new, a_new, Q.get(s_new,a_new))\n",
    "\n",
    "        new_Qa = Q.get(s,a) + 0.3*(reward + 0.9*Q.get(s_new,a_new) - Q.get(s,a))         \n",
    "        Q.set(s,a,new_Qa)\n",
    "        \n",
    "        s,a = s_new,a_new\n",
    "        \n",
    "    iepisode += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"State Value Function evaluation for Random Policy\")\n",
    "\n",
    "for key, value in sorted(Q.f.items()):\n",
    "    print(key, [round(v,2) for v in value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Optimal Policy through epsilon - Greedy Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class egreedy_Class():\n",
    "    \"\"\" Class for the epsilon greedy policy\n",
    "        Gets at initialization:\n",
    "        -environment to store the number of actions in initialization \n",
    "        -The Q function\n",
    "        \n",
    "        Has method act: \n",
    "        -gets state and epsilon as input: acts randomly \n",
    "        -\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, Q):       \n",
    "        self.nactions = env.action_space.n\n",
    "        self.Q = Q\n",
    "    \n",
    "    def act(self, s, epsilon):                \n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            q_values = self.Q.get(s)\n",
    "            action = np.argmax(q_values)\n",
    "            return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Value Function evaluation for Random Policy\n",
      "0 [-8.93, -8.37, -10.01, -9.21]\n",
      "1 [-8.79, -7.97, -9.19, -9.09]\n",
      "2 [-8.95, -7.64, -9.05, -9.07]\n",
      "3 [-8.4, -7.32, -8.66, -8.51]\n",
      "4 [-8.2, -7.06, -10.55, -8.38]\n",
      "5 [-7.42, -6.83, -11.29, -7.99]\n",
      "6 [-6.93, -6.37, -7.42, -7.96]\n",
      "7 [-6.34, -5.89, -13.39, -7.63]\n",
      "8 [-6.05, -5.01, -7.53, -6.74]\n",
      "9 [-5.6, -4.29, -6.56, -6.58]\n",
      "10 [-5.14, -3.5, -5.0, -5.9]\n",
      "11 [-4.3, -4.07, -2.72, -5.55]\n",
      "12 [-8.74, -9.17, -10.76, -10.24]\n",
      "13 [-8.49, -10.77, -20.0, -10.89]\n",
      "14 [-8.27, -9.89, -16.38, -11.06]\n",
      "15 [-7.92, -10.26, -15.96, -11.41]\n",
      "16 [-8.42, -11.97, -43.84, -11.25]\n",
      "17 [-9.13, -11.75, -23.59, -11.84]\n",
      "18 [-6.73, -12.11, -52.92, -12.97]\n",
      "19 [-6.35, -12.24, -23.73, -12.66]\n",
      "20 [-9.12, -5.74, -36.59, -15.76]\n",
      "21 [-7.83, -4.09, -11.14, -11.43]\n",
      "22 [-7.58, -3.24, -12.71, -8.55]\n",
      "23 [-4.08, -3.09, -1.9, -4.44]\n",
      "24 [-8.94, -15.6, -16.58, -9.83]\n",
      "25 [-8.99, -27.02, -100.0, -13.05]\n",
      "26 [-12.15, -15.22, -100.0, -29.78]\n",
      "27 [-11.57, -36.37, -100.0, -16.92]\n",
      "28 [-17.13, -21.31, -99.89, -24.97]\n",
      "29 [-13.99, -22.03, -99.32, -34.03]\n",
      "30 [-12.46, -32.35, -99.03, -35.52]\n",
      "31 [-9.89, -30.33, -94.24, -35.32]\n",
      "32 [-9.59, -13.38, -99.67, -12.79]\n",
      "33 [-6.52, -24.51, -95.96, -21.59]\n",
      "34 [-7.17, -2.32, -97.18, -27.81]\n",
      "35 [-3.87, -2.07, -1.0, -4.38]\n",
      "36 [-9.2, -100.0, -14.14, -12.64]\n",
      "37 [0, 0, 0, 0]\n",
      "38 [0, 0, 0, 0]\n",
      "39 [0, 0, 0, 0]\n",
      "40 [0, 0, 0, 0]\n",
      "41 [0, 0, 0, 0]\n",
      "42 [0, 0, 0, 0]\n",
      "43 [0, 0, 0, 0]\n",
      "44 [0, 0, 0, 0]\n",
      "45 [0, 0, 0, 0]\n",
      "46 [0, 0, 0, 0]\n",
      "47 [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "env = Environment.Environment(\"CliffWalking\")\n",
    "Q = Q_Class(env)\n",
    "policy = egreedy_Class(env,Q) \n",
    "\n",
    "iepisode, epsilon = 0,1\n",
    "while iepisode < 1000:\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    a = policy.act(s,epsilon)\n",
    "    while not done:        \n",
    "        s_new, reward, done, info = env.step(a)        \n",
    "        a_new = policy.act(s_new,epsilon)\n",
    "        new_Qa = Q.get(s,a) + 0.3*(reward + 0.9*Q.get(s_new,a_new) - Q.get(s,a))         \n",
    "        Q.set(s,a,new_Qa)\n",
    "        s,a = s_new,a_new\n",
    "        \n",
    "    iepisode += 1\n",
    "    epsilon = max(0.1, epsilon - 1/1000.0)\n",
    "\n",
    "print(\"\")\n",
    "print(\"State Value Function evaluation for Random Policy\")\n",
    "\n",
    "for key, value in sorted(Q.f.items()):\n",
    "    print(key, [round(v,2) for v in value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning - Off-policy through 1 step look ahead - max operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Value Function evaluation for Random Policy\n",
      "0 [-9.54, -9.53, -9.55, -9.54]\n",
      "1 [-9.49, -9.49, -9.51, -9.52]\n",
      "2 [-9.46, -9.44, -9.44, -9.44]\n",
      "3 [-9.39, -9.38, -9.4, -9.39]\n",
      "4 [-9.31, -9.3, -9.31, -9.34]\n",
      "5 [-9.23, -9.22, -9.24, -9.21]\n",
      "6 [-9.15, -9.15, -9.15, -9.17]\n",
      "7 [-9.07, -9.07, -9.09, -9.08]\n",
      "8 [-9.04, -9.0, -9.02, -9.0]\n",
      "9 [-8.92, -8.9, -8.91, -8.96]\n",
      "10 [-8.85, -8.85, -8.87, -8.84]\n",
      "11 [-8.81, -8.8, -8.81, -8.8]\n",
      "12 [-9.55, -9.55, -9.57, -9.55]\n",
      "13 [-9.52, -9.5, -9.51, -9.52]\n",
      "14 [-9.45, -9.45, -9.45, -9.5]\n",
      "15 [-9.41, -9.39, -9.41, -9.41]\n",
      "16 [-9.32, -9.31, -9.32, -9.42]\n",
      "17 [-9.23, -9.23, -9.24, -9.33]\n",
      "18 [-9.16, -9.16, -9.2, -9.24]\n",
      "19 [-9.08, -9.07, -9.1, -9.14]\n",
      "20 [-9.0, -9.01, -9.02, -9.01]\n",
      "21 [-8.91, -8.92, -8.93, -8.96]\n",
      "22 [-8.84, -8.81, -8.83, -8.84]\n",
      "23 [-8.74, -8.74, -8.73, -8.73]\n",
      "24 [-9.58, -9.58, -9.6, -9.61]\n",
      "25 [-9.53, -9.53, -100.0, -9.56]\n",
      "26 [-9.48, -9.47, -100.0, -9.5]\n",
      "27 [-9.43, -9.41, -100.0, -9.5]\n",
      "28 [-9.36, -9.35, -100.0, -9.39]\n",
      "29 [-9.28, -9.26, -100.0, -9.33]\n",
      "30 [-9.18, -9.18, -99.99, -9.23]\n",
      "31 [-9.11, -9.1, -100.0, -9.1]\n",
      "32 [-9.03, -9.01, -100.0, -9.09]\n",
      "33 [-8.92, -8.87, -99.94, -8.93]\n",
      "34 [-8.75, -8.75, -100.0, -8.82]\n",
      "35 [-8.61, -8.29, -1.0, -8.39]\n",
      "36 [-9.62, -100.0, -9.63, -9.63]\n",
      "37 [0, 0, 0, 0]\n",
      "38 [0, 0, 0, 0]\n",
      "39 [0, 0, 0, 0]\n",
      "40 [0, 0, 0, 0]\n",
      "41 [0, 0, 0, 0]\n",
      "42 [0, 0, 0, 0]\n",
      "43 [0, 0, 0, 0]\n",
      "44 [0, 0, 0, 0]\n",
      "45 [0, 0, 0, 0]\n",
      "46 [0, 0, 0, 0]\n",
      "47 [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Notes about Q learning.\n",
    "#Q-learning means that the maximum future reward for an action is the immediate reward plus\n",
    "#the maximum future reward for the next state.\n",
    "#The Q table is initialized randomly, the agent then interacts with the environment.\n",
    "#From interacting with the environment the agent will observe the reward that came from its action \n",
    "#and the transition that took place in the state.\n",
    "#The agent will then compute the observed Q- value and update its own estimate of Q. \n",
    "\n",
    "env = Environment.Environment(\"CliffWalking\")\n",
    "\n",
    "#The Q table is initialized randomly.\n",
    "Q = Q_Class(env)\n",
    "policy = egreedy_Class(env,Q) \n",
    "\n",
    "#the agent then interacts with the environment\n",
    "iepisode,epsilon = 0,1\n",
    "while iepisode < 1000:\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #From interacting with the environment the agent will observe the reward that came from its action \n",
    "    #and the transition that took place in the state.\n",
    "    while not done:        \n",
    "        a = policy.act(s,epsilon)\n",
    "        s_new, reward, done, info = env.step(a)\n",
    "\n",
    "        #The agent will then compute the observed Q- value and update its own estimate of Q. \n",
    "        new_Qa = Q.get(s,a) + 0.3*(reward + 0.9*Q.get(s_new,a_new) - Q.get(s,a))         \n",
    "        Q.set(s,a,new_Qa)\n",
    "        \n",
    "        s,a = s_new,a_new\n",
    "        \n",
    "    iepisode += 1\n",
    "    epsilon = max(0.1, epsilon - 1/1000.0)\n",
    "\n",
    "print(\"\")\n",
    "print(\"State Value Function evaluation for Random Policy\")\n",
    "\n",
    "for key, value in sorted(Q.f.items()):\n",
    "    print(key, [round(v,2) for v in value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
